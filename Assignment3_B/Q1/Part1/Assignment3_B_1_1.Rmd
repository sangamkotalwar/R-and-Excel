---
title: "Assignment3_B_1_1"
author: "Sangamesh"
date: "25 September 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Reading data and making data into training and test data.
```{r echo=F, message=F, error=F, warning=F}
RawData <- read.table("RegD1.txt", header=T)
#Considering every tenth data
test = seq(10,nrow(RawData),by=10)
#trainData
trainData <- RawData[-test,]
print("head of training data")
print(head(trainData))
#testData
testData <- RawData[test,]
print("head of testing data")
print(head(testData))
```

Fitting data for linear regression model
```{r echo=F, message=F, error=F, warning=F}

model <- lm(Y1~X1+X2, data=trainData)
summary(model)
```

Several metrics useful for regression diagnostics : model.diag.metrics
```{r echo=F, message=F, error=F, warning=F}
library(broom)
model.diag.metrics <- augment(model)
head(model.diag.metrics)
```

Meta Data for model.diag.metrics
Among the table columns, there are:

Y1: original values
X1, X2: the observed values
.fitted: the fitted values
.resid: the residual errors


Let's see correlation between the features:
```{r echo=F, message=F, error=F, warning=F}
library(corrplot)
#corrplot(trainData, method = "number")
```
We can see the plot in residual and fitted plot here now:
```{r echo=F, message=F, error=F, warning=F}
plot(model, pch=16, which=1)
```

Note how the residuals plot of this last model shows some important points still lying far away from the middle area of the graph. Since the behaviour is random in nature we were successful in this test.

```{}
###Letâ€™s visualize a three-dimensional interactive graph with both predictors and the target variable and the linear fit from the last model:###
```


```{r}
#library(car)
library(stats)
qqnorm(residuals(model))
qqline(residuals(model))
```

Check outliers:
```{r}
library(car)
leveragePlots(model)
```

We can see that there are outliers in this dataset mainly row 5,42,57,32 from X1, and 18,42,57,1 from X2. So, overall number 42 and 57 are outliers.

Influential Variables :
Fiding influential data points is required. 
So let us look at variable plots:
```{r echo=F, message=F, warning=F, error=FALSE}
# Cook's D plot
# identify D values > 4/(n-k-1 )
cutoff <- 4/((nrow(trainData)-length(model$coefficients)-2)) 
plot(model, which=4, cook.levels=cutoff)
# It shows datapoints 1,5 and 47 are influential
influencePlot(model, main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```



---
title: 'B. Regression - I : Question 3'
author: "Author: Sangamesh"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---
  
  Question: Use the data to fit a model using the following methods.
(a) Least squares.
(b) Least absolute deviations.
(c) Huber method.
(d) Least trimmed squares.
Compare the results. Use diagnostic methods to detect any outliers or influential points. Remove these points and then use least squares. Compare the results.
-----------------------------------------------------------------------------------------------------------------

####(a) Least squares.
  \hfill\break  
  Least squares is a statistical method used to determine a line of best fit by minimizing the sum of squares created by a mathematical function. A "square" is determined by squaring the distance between a data point and the regression line. The least squares approach limits the distance between a function and the data points that a function is trying to explain. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data.
  \hfill\break
  
```{r data_load,,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
library(dplyr)
data_regd5 = read.table("RegD4.txt", header=TRUE)

# Divding the dataset into train and test set.
train5=slice(data_regd5, seq(-10, -nrow(data_regd5), -10))
test5=slice(data_regd5, seq(10, nrow(data_regd5), 10))

# Linear Model
lm_model_reg5=lm(Y1~.,train5)
summary(lm_model_reg5)
```
####(b) Least absolute deviations.
  \hfill\break 
The method of least absolute deviations fits a line to a set of (x,y) data by choosing slope and intercept parameters to minimize the SAE, or the sum of absolute errors. As with the SSE associated with least squares, the errors in question are the differences between the actual y data values and the corresponding y values defined by the line.
  \hfill\break 
```{r,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
library (quantreg)
# Linear Model
model_reg5=rq(Y1~.,data=train5)
summary(model_reg5)
```

####(c) Huber method.

```{r,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
library (MASS)
# Linear Model
model_reg5=rlm(Y1~.,data=train5)
summary(model_reg5)
```

####(c) Least trimmed squares.
Least trimmed squares (LTS), or least trimmed sum of squares, is a robust statistical method that fits a function to a set of data whilst not being unduly affected by the presence of outliers. It is one of a number of methods for robust regression. Instead of the standard least squares method, which minimises the sum of squared residuals over n points, the LTS method attempts to minimise the sum of squared residuals over a subset k of those points. The unused n-k points do not influence the fit.

```{r,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
library (robustbase)
# Linear Model
model_reg5=ltsReg(Y1~.,data=train5)
summary(model_reg5)
```
   
  \hfill\break  
  
  **Inference: In all the applied regression methods the variable X3 is consistently insignificant therefore it can be removed. **
  **There doesn't seem to be a significant change in the coefficient of other variables when compared across the techniques. So, its better to use least squares method.**
  \hfill\break
  
####DIAGNOSIS TO FIND INFLUENTIAL POINTS AND OUTLIERS  
  \hfill\break
  *Influential Points * 
  \hfill\break
  An influential value is a value, which inclusion or exclusion can alter the results of the regression analysis. Such a value is associated with a large residual.  
  \hfill\break  
  Statisticians have developed a metric called Cook's distance to determine the influence of a value. This metric defines influence as a combination of leverage and residual size.  
    A rule of thumb is that an observation has high influence if Cook's distance exceeds 4/(n - p - 1), where n is the number of observations and p the number of predictor variables.
    \hfill\break  
  
```{r, fig.width=4, fig.height=4,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
# Cook's distance
plot(lm_model_reg5,4,id.n = 1)
```

**Inference: From above plot we can see there is one point(#19) exceeding Cook's distance(in our case, 0.26).  **\
 
\hfill\break
  *Outliers *
  \hfill\break  
  Outliers can be identified by examining the standardized residual (or studentized residual), which is the residual divided by its estimated standard error. Standardized residuals can be interpreted as the number of standard errors away from the regression line.  
  Observations whose standardized residuals are greater than 3 in absolute value are possible outliers.

```{r, fig.width=4, fig.height=4,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
# Residuals vs Leverage
plot(lm_model_reg5, 5,id.n = 3)
```

**Inference: Observation #19 and #4 are outliers.**

####Computing least squares after removal of the outlier
```{r, fig.width=4, fig.height=4,warning=FALSE,message=FALSE,error=FALSE,echo=FALSE}
# Linear Model
lm_model_reg5=lm(Y1~.,train5[-c(19,4),])
summary(lm_model_reg5)
```
  
  **Inference: After removal of outliers, the value of R-squared increased from 0.915 to 0.9707and Adjusted R-squared increased from 0.898 to  0.964.**
  \hfill\break
  **Also, the significance of predictor X2 reduced.**